# -*- coding: utf-8 -*-
"""ass3Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GrUvhmGoHl3yd8QYILm0FMwWpwP2RJWI
"""

import numpy as np
import sys
import pdb
import pandas as pd
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPClassifier

X_train = np.load("/content/x_train.npy")
y_train = np.load("/content/y_train.npy")
X_test = np.load("/content/x_test.npy")
y_test = np.load("/content/y_test.txt")
X_train = X_train.astype('float')
y_train = y_train.astype('float')
X_train = 2*(0.5 - X_train/255)
X_test = 2*(0.5 - X_test/255)
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.fit_transform(X_test)
# print(X_test)

label_encoder = OneHotEncoder(sparse_output = False)
label_encoder.fit(np.expand_dims(y_train, axis = -1))
y_train_onehot = label_encoder.transform(np.expand_dims(y_train, axis = -1))
y_test_onehot = label_encoder.transform(np.expand_dims(y_test, axis = -1))

def sigmoid(x):
    return 1/(1+np.exp(-x))

def Reluu(x):
    return np.maximum(0, x)

def forward_prop(hidden_layers,X,y,W,relu):
    O_all = np.empty(len(hidden_layers)+1, dtype=object)
    for L in range(len(hidden_layers)+1):
        if L==0:
            if relu==True:
                O_all[L] = Reluu(X @ W[L])
            else:
                O_all[L] = sigmoid(X @ W[L])
        elif L==len(hidden_layers):
            Dot = O_all[L-1] @ W[L]
            O_all[L] = np.zeros(Dot.shape)
            for i in range(len(Dot)):
                xm=np.argmax(Dot[i])
                expp=np.exp(Dot[i]-Dot[xm])
                s=np.sum(expp)
                O_all[L][i] = expp/s
        else:
            if relu==True:
                O_all[L] = Reluu(O_all[L-1] @ W[L])
            else:
                O_all[L] = sigmoid(O_all[L-1] @ W[L])
    return O_all

def backward_prop(hidden_layers,X,y,O_all,W,y_pred,learn,itr,relu):
    Delta_all = np.empty(len(hidden_layers)+1, dtype=object)
    for L in range(len(hidden_layers),-1,-1):
        if L==len(hidden_layers):
            Delta_all[-1] = O_all[-1]-y
        else:
            if relu==True:
                Delta_all[L] = (O_all[L]>0).astype(int) * (Delta_all[L+1] @ W[L+1].T)
            else:
                Delta_all[L] = O_all[L] * (1-O_all[L]) * (Delta_all[L+1] @ W[L+1].T)
    Learning_rate = 0.01
    if learn==True:
        Learning_rate/=itr+1
    for L in range(len(hidden_layers)+1):
        if L==0:
            W[L] -= Learning_rate* (X.T @ Delta_all[0])/len(X)
        else:
            W[L] -= Learning_rate* (O_all[L-1].T @ Delta_all[L])/len(O_all[L-1])
    return W

def IntialiseW(hidden_layers,X,y):
    W = np.empty(len(hidden_layers)+1, dtype=object)
    for L in range(len(hidden_layers)+1):
        if L==0:
            W[L] = np.random.rand(len(X[0]),hidden_layers[0])*np.sqrt(2/(len(X[0])+hidden_layers[0]))
        elif L==len(hidden_layers):
            W[L] = np.random.rand(hidden_layers[L-1],len(y[0]))*np.sqrt(2/(hidden_layers[L-1]+len(y[0])))
        else:
            W[L] = np.random.rand(hidden_layers[L-1],hidden_layers[L])*np.sqrt(2/(hidden_layers[L-1]+hidden_layers[L]))
    return W

def train_neural_network(hidden_layers,M,X,y,tol,max_itr,learn,relu):
    X = np.hstack([np.ones((len(y),1)),X])
    W = IntialiseW(hidden_layers,X,y)
    c=0
    prv = 1000
    for itr in range(max_itr):
        J_theta = 0
        Out = forward_prop(hidden_layers,X,y,W,relu)
        for i in range(len(y)):
            for j in range(len(y[0])):
                if y[i][j]==1:
                    J_theta += -np.log2(Out[-1][i][j])
        J_theta/=len(y)
        if prv-J_theta<tol:
            break
        prv = J_theta
        for i in range(0,len(y),M):
            X_batch = X[i:i+M]
            y_batch = y[i:i+M]
            Out = forward_prop(hidden_layers,X_batch,y_batch,W,relu)
            y_pred = np.zeros(Out[-1].shape)
            pred = np.array([np.argmax(Out[-1][i]) for i in range(len(Out[-1]))])
            for i in range(len(y_pred)):
                y_pred[i][pred[i]] = 1
            W = backward_prop(hidden_layers,X_batch,y_batch,Out,W,y_pred,learn,itr,relu)
    return W

def testing(W,X_test,y_test,hidden_layers,relu):
    X_test = np.hstack([np.ones((len(X_test),1)),X_test])
    Out = forward_prop(hidden_layers,X_test,y_test,W,relu)
    y_pred = np.zeros(Out[-1].shape)
    pred = np.array([np.argmax(Out[-1][i]) for i in range(len(Out[-1]))])
    pred+=1
    for i in range(len(y_pred)):
        y_pred[i][pred[i]-1] = 1
    # print(classification_report(pred,y_test))
    print(accuracy_score(pred,y_test),"accuracy")
    print(f1_score(pred,y_test,average='macro'),"F1-score")
    print(precision_score(pred,y_test,average='macro'),"Precision")
    print(recall_score(pred,y_test,average='macro'),"Recall")
    return f1_score(pred,y_test,average='macro')

F1_scores = []
all = np.array([[1],[5],[10],[50],[100]])
for hid in all:
    W = train_neural_network(hid,32,X_train,y_train_onehot,10**(-5),10,False,False)
    print("hidden layer is",hid)
    print("Over Train set")
    f1_train = testing(W,X_train,y_train,hid,False)
    print("Over Test set")
    f1_test = testing(W,X_test[:1000],y_test,hid,False)
    F1_scores.append(f1_test)
plt.figure()
plt.plot(all[:,0],F1_scores)
plt.title("F1 scores over different max depths")
plt.xlabel("hidden units")
plt.ylabel("f1 score")
plt.show()

F1_scores = []
all = [[512],[512,256],[512,256,128],[512,256,128,64]]
oii = [1,2,3,4]
for hid in all:
    W = train_neural_network(hid,32,X_train,y_train_onehot,10**(-10),30,False,False)
    print("hidden layer is",hid)
    print("Over Train set")
    f1_train = testing(W,X_train,y_train,hid,False)
    print("Over Test set")
    f1_test = testing(W,X_test[:1000],y_test,hid,False)
    F1_scores.append(f1_test)
plt.figure()
plt.plot(oii,F1_scores)
plt.title("F1 scores over different network depths")
plt.xlabel("network depth")
plt.ylabel("f1 score")
plt.show()

F1_scores = []
all = [[512],[512,256],[512,256,128],[512,256,128,64]]
oii = [1,2,3,4]
for hid in all:
    W = train_neural_network(hid,32,X_train,y_train_onehot,10**(-10),20,True,False)
    print("hidden layer is",hid)
    print("Over Train set")
    f1_train = testing(W,X_train,y_train,hid,False)
    print("Over Test set")
    f1_test = testing(W,X_test[:1000],y_test,hid,False)
    F1_scores.append(f1_test)
plt.figure()
plt.plot(oii,F1_scores)
plt.title("F1 scores over different network depths")
plt.xlabel("network depth")
plt.ylabel("f1 score")
plt.show()

F1_scores = []
all = [[512],[512,256],[512,256,128],[512,256,128,64]]
oii = [1,2,3,4]
for hid in all:
    W = train_neural_network(hid,32,X_train,y_train_onehot,10**(-10),20,True,True)
    print("hidden layer is",hid)
    print("Over Train set")
    f1_train = testing(W,X_train,y_train,hid,False)
    print("Over Test set")
    f1_test = testing(W,X_test[:1000],y_test,hid,False)
    F1_scores.append(f1_test)
plt.figure()
plt.plot(oii,F1_scores)
plt.title("F1 scores over different network depths in Relu activation")
plt.xlabel("network depth")
plt.ylabel("f1 score")
plt.show()

F1_scores = []
all = [[512],[512,256],[512,256,128],[512,256,128,64]]
oii = [1,2,3,4]
for hid in all:
    clf = MLPClassifier(hidden_layer_sizes=hid,activation='relu',solver='sgd',alpha=0,batch_size=32,learning_rate='invscaling',max_iter=20,random_state=0)
    clf.fit(X_train, y_train)
    print("hidden layer is",hid)
    print("Over Train set")
    pred_train = clf.predict(X_train)
    precision = precision_score(y_train,pred_train, average='macro')
    recall = recall_score(y_train,pred_train, average='macro')
    f1 = f1_score(y_train,pred_train, average='macro')
    print(precision,"precision")
    print(recall,"recall")
    print(f1,"f1 score")
    print("Over Test set")
    pred_test = clf.predict(X_test[:1000])
    precision = precision_score(y_test,pred_test, average='macro')
    recall = recall_score(y_test,pred_test, average='macro')
    f1_test = f1_score(y_test,pred_test, average='macro')
    F1_scores.append(f1_test)
    print(precision,"precision")
    print(recall,"recall")
    print(f1_test,"f1 score")
plt.figure()
plt.plot(oii,F1_scores)
plt.title("F1 scores over different network depths using sklearn")
plt.xlabel("network depth")
plt.ylabel("f1 score")
plt.show()
